import os
import sys

# ğŸ“š Pythonæ¨¡å—ç³»ç»Ÿ
__package__ = "trainer"
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import argparse  # å‘½ä»¤è¡Œå‚æ•°è§£æ
import re        # æ­£åˆ™è¡¨è¾¾å¼ï¼Œç”¨äºå¥–åŠ±è®¡ç®—
import warnings  # è­¦å‘Šæ§åˆ¶
import torch     # PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶
import torch.distributed as dist  # åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ
import torch.nn.functional as F   # ç¥ç»ç½‘ç»œå‡½æ•°
from transformers import AutoTokenizer  # HuggingFaceåˆ†è¯å™¨
from contextlib import nullcontext  # ä¸Šä¸‹æ–‡ç®¡ç†å™¨
from torch import optim, nn  # ä¼˜åŒ–å™¨å’Œç¥ç»ç½‘ç»œ
from torch.nn.parallel import DistributedDataParallel  # åˆ†å¸ƒå¼å¹¶è¡Œ
from torch.utils.data import DataLoader, DistributedSampler  # æ•°æ®åŠ è½½
from torch.nn.utils import clip_grad_norm_  # æ¢¯åº¦è£å‰ª
from torch.optim.lr_scheduler import CosineAnnealingLR  # ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦
from transformers import AutoModel  # HuggingFaceæ¨¡å‹åŠ è½½
from model.MokioModel import MokioMindConfig, MokioMindForCausalLM  # MiniMindæ¨¡å‹
from dataset.lm_dataset import RLAIFDataset  # RLæ•°æ®é›†
from trainer.trainer_utils import (  # è®­ç»ƒå·¥å…·å‡½æ•°
    Logger, is_main_process, lm_checkpoint, init_distributed_mode, 
    setup_seed, SkipBatchSampler, init_model
)

warnings.filterwarnings('ignore')
#==========Critic Modeléƒ¨åˆ†==========

class CriticModel(MokioMindForCausalLM):
    def __init__(self,params):
        super().__init__(params)
        # ä»·å€¼å¤´ï¼Œç”¨äºè¾“å‡ºæ¯ä¸ªtokenä½ç½®çš„çŠ¶æ€ä»·å€¼
        self.value_head=nn.Linear(params.hidden_size,1)
        
    def forward(self,input_ids=None,attention_mask=None,**kwargs):
        outputs=self.model(input_ids=input_ids,attention_mask=attention_mask,**kwargs)
        hidden_states=self.model.norm(outputs[0])
        
        values=self.value_head(hidden_states).squeeze(-1)
        return values

#==========å¥–åŠ±è®¡ç®—éƒ¨åˆ†==========
def calculate_rewards(prompts,responses,reward_model,reward_tokenizer):
    def reasoning_model_reward(rewards):
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æ€è€ƒ-å›ç­”æ ¼å¼
        pattern = r"^<think>\n.*?\n</think>\n<answer>\n.*?\n</answer>$"
        # å¤šäº†ä¸€ä¸ª\nï¼Œè€ƒè™‘åˆ°thinkå’Œanswerä¹‹é—´æœ‰ç©ºè¡Œçš„æƒ…å†µ
        pattern2 = r"^<think>\n.*?\n</think>\n\n<answer>\n.*?\n</answer>$"
        # é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼è®¡ç®—å¥–åŠ±ï¼Œå¦‚æœå›ç­”ç¬¦åˆæ ¼å¼åˆ™å¥–åŠ±0.5ï¼Œå¦åˆ™0.0
        matches_pattern = [re.match(pattern, response, re.S) for response in responses]
        matches_pattern2 = [re.match(pattern2, response, re.S) for response in responses]
        
        format_rewards = []
        for match_pattern, match_pattern2 in zip(matches_pattern, matches_pattern2):
            if match_pattern:
                format_rewards.append(0.5)
            elif match_pattern2:
                format_rewards.append(0.5)
            else:
                format_rewards.append(0.0)
        rewards += torch.tensor(format_rewards, device=args.device)
        
        def mark_num(text):
            reward=0
            if text.count("<think>")==1:
                reward+=0.25
            if text.count("</think>")==1:
                reward+=0.25
            if text.count("<answer>")==1:
                reward+=0.25
            if text.count("</answer>")==1:
                reward+=0.25
            return reward
        
        mark_rewards=[mark_num(response) for response in responses]
        rewards+=torch.tensor(mark_rewards,device=args.device)
        return rewards
    rewards=torch.zeros(len(responses),device=args.device)
    
    if args.reasoning==1:
        rewards=reasoning_model_reward(rewards)
#==========Rewardæ¨¡å‹è¯„åˆ†éƒ¨åˆ†==========
    with torch.no_grad():
        reward_model_scores = []
        for prompt,response in zip(prompts,responses):
            
            pattern = r"<\|im_start\|>(system|user|assistant)\s+(.*?)<\|im_end\|>"
            matches = re.findall(pattern, prompt, re.DOTALL)
            messages = [{"role": role, "content": content.strip()} for role, content in matches]
            
            tmp_chat=messages+[{"role":"assistant","content":response}]
            score=reward_model.get_reward(tmp_chat,reward_tokenizer)
            
            scale=3.0
            score=max(min(score,scale),-scale)
            
            if args.reasoning==1:
                answer_match = re.search(r'<answer>(.*?)</answer>', response, re.DOTALL)
                if answer_match:
                    answer_content = answer_match.group(1).strip()
                    # å¯¹answerå†…å®¹å•ç‹¬è®¡ç®—reward
                    tmp_chat = messages + [{"role": "assistant", "content": answer_content}]
                    answer_score = reward_model.get_score(reward_tokenizer, tmp_chat)
                    answer_score = max(min(answer_score, scale), -scale)
                    # ğŸ“š åŠ æƒç»„åˆ
                    score = score * 0.4 + answer_score * 0.6
            reward_model_scores.append(score)
        
        reward_model_scores=torch.tensor(reward_model_scores,device=args.device)
        rewards+=reward_model_scores
        
    return rewards

#==========PPOè®­ç»ƒä¸€ä¸ªEpochéƒ¨åˆ†==========
def ppo_train_epoch(epoch, loader, iters, old_actor_model, ref_model, actor_scheduler, critic_scheduler, reward_model, reward_tokenizer, start_step=0, wandb=None):
    # åˆ‡æ¢actorå’Œcriticæ¨¡å‹åˆ°è®­ç»ƒæ¨¡å¼
    actor_model.train()
    critic_model.train()
    
    for step,batch in enumerate(loader,start=start_step+1):
        prompts=batch['prompt']
        # ç¼–ç è¾“å…¥
        enc=tokenizer(prompts,return_tensors='pt',padding=True,truncation=True,max_length=args.max_seq_len).to(args.device)
        # è®¡ç®—æ¯ä¸ªpromptçš„é•¿åº¦ï¼ˆç”¨äºåç»­å¤„ç†ï¼‰
        prompt_lengths=enc.attention_mask.sum(dim=1)
        
        with torch.no_grad():
            model_for_gen=actor_model.module if isinstance(actor_model,DistributedDataParallel) else actor_model
            
            gen_out=model_for_gen.generate(
                input_ids=enc.input_ids,
                attention_mask=enc.attention_mask,
                max_new_tokens=args.max_gen_len,
                do_sample=True,
                temperature=0.8,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        # è§£ç ç”Ÿæˆçš„å“åº”
        responses_text=[tokenizer.decode(gen_out[i,prompt_lengths[i]:],skip_special_tokens=True) for i in range(len(prompts))]
        
        # è®¡ç®—å¥–åŠ±
        rewards=calculate_rewards(prompts,responses_text,reward_model,reward_tokenizer)
        
        # åˆ›å»ºä¸€ä¸ªmaskï¼Œç”¨äºæ ‡è®°å“ªäº›ä½ç½®ä¸Šæ˜¯æœ‰æ•ˆtoken
        full_mask=(gen_out!=tokenizer.pad_token_id).long()
        # criticæ¨¡å‹è¿›è¡Œä»·å€¼ä¼°è®¡
        value_seq=critic_model(input_ids=gen_out,attention_mask=full_mask)
        # æ‹¿åˆ°æœ€åä¸€ä¸ªépadä½ç½®çš„ç´¢å¼•
        last_indices=full_mask.sum(dim=1)-1
        # è·å–æ¯æ¡åºåˆ—æœ€åtokençš„value
        values=value_seq[torch.arange(len(last_indices)),last_indices]
        # advantage=reward-ä¼°è®¡çš„value
        advantages = rewards - values.detach()  # [B]
        
        # è®¡ç®—actor logï¼Œè¡¨ç¤ºactorå¯¹è¿™ä¸ªç­”æ¡ˆçš„â€œä¿¡å¿ƒâ€
        # å…ˆç”Ÿæˆlogits
        logits=actor_model(input_ids=gen_out,attention_mask=full_mask).logits  # [B, L, V]
        # labelæ˜¯ç”Ÿæˆçš„tokenåºåˆ—ï¼Œå»æ‰ç¬¬ä¸€ä¸ªtokenï¼ˆå› ä¸ºlogitsæ˜¯é¢„æµ‹ä¸‹ä¸€ä¸ªtokençš„æ¦‚ç‡ï¼‰
        labels=gen_out[:,1:].clone()
        # ä½¿ç”¨log_softmaxè®¡ç®—logæ¦‚ç‡
        logp_tokens=F.log_softmax(logits[:,:-1,:],dim=-1).gather(2,labels.unsqueeze(-1)).squeeze(-1)  # [B, L-1]
        seq_len=gen_out.size(1)-1
        # åªå…³å¿ƒresponseéƒ¨åˆ†çš„æ¦‚ç‡ï¼Œæ‰€ä»¥è¦æŠŠpromptséƒ¨åˆ†çš„maskæ‰
        resp_mask=torch.arange(seq_len,device=gen_out.device).unsqueeze(0)>=prompt_lengths.unsqueeze(1)
        
        final_mask=resp_mask&(~labels.eq(tokenizer.pad_token_id))
        # æŠŠæ‰€æœ‰responseéƒ¨åˆ†çš„logæ¦‚ç‡åŠ èµ·æ¥ï¼Œå¾—åˆ°æ¯æ¡åºåˆ—çš„æ€»logæ¦‚ç‡
        actor_logp=(logp_tokens*final_mask).sum(dim=1)

        # è®¡ç®—oldå’Œref logçš„æ¦‚ç‡
        # oldç”¨äºé˜²æ­¢ç­–ç•¥æ›´æ–°è¿‡å¤§ï¼Œrefç”¨äºè®¡ç®—KLæƒ©ç½šï¼Œé˜²æ­¢æ¨¡å‹å¿˜æœ¬
        with torch.no_grad():
            old_logits = old_actor_model(input_ids=gen_out, attention_mask=full_mask).logits  # [B, P+R, V]
            old_logp_tokens = F.log_softmax(old_logits[:, :-1], dim=-1).gather(2, labels.unsqueeze(-1)).squeeze(-1)  # [B, P+R-1]
            old_logp = (old_logp_tokens * final_mask).sum(dim=1)  # [B]
            
            ref_logits = ref_model(input_ids=gen_out, attention_mask=full_mask).logits  # [B, P+R, V]
            ref_logp_tokens = F.log_softmax(ref_logits[:, :-1], dim=-1).gather(2, labels.unsqueeze(-1)).squeeze(-1)  # [B, P+R-1]
            ref_logp = (ref_logp_tokens * final_mask).sum(dim=1)  # [B]
            
        # è®¡ç®—KLæ•£åº¦å’Œratio
        kl=(actor_logp - old_logp).mean()
        kl_ref=(actor_logp - ref_logp).mean()
        ratio=torch.exp(actor_logp - old_logp)  # [B]
        
        # PPOè£å‰ªæŸå¤±
        surr1=ratio*advantages  # [B]
        surr2=torch.clamp(ratio,1.0 - args.clip_epsilon, 1.0 + args.clip_epsilon)*advantages  # [B]
        policy_loss=-torch.min(surr1,surr2).mean()
        
        # ä»·å€¼å‡½æ•°æŸå¤±
        value_loss=F.mse_loss(values,rewards)
        # æ€»æŸå¤±
        loss = policy_loss + args.vf_coef * value_loss + args.kl_coef * kl_ref  # scalar
        loss.backward()
        
        # æ›´æ–°å‚æ•°
        if (step + 1) % args.accumulation_steps == 0:
            clip_grad_norm_(actor_model.parameters(), args.grad_clip)
            clip_grad_norm_(critic_model.parameters(), args.grad_clip)
            actor_optimizer.step()
            critic_optimizer.step()
            actor_scheduler.step()
            critic_scheduler.step()
            actor_optimizer.zero_grad()
            critic_optimizer.zero_grad()
            
        # ğŸ“š æ—¥å¿—è®°å½•
        if is_main_process():
            response_ids = gen_out[:, enc.input_ids.shape[1]:]
            is_eos = (response_ids == tokenizer.eos_token_id)
            eos_indices = torch.argmax(is_eos.int(), dim=1)
            has_eos = is_eos.any(dim=1)
            lengths = torch.where(has_eos, eos_indices + 1, torch.tensor(response_ids.shape[1], device=is_eos.device))
            avg_len = lengths.float().mean()

            actor_loss_val = policy_loss.item()
            critic_loss_val = value_loss.item()
            reward_val = rewards.mean().item()
            kl_val = kl.item()
            kl_ref_val = kl_ref.item()
            avg_len_val = avg_len.item()
            actor_lr = actor_optimizer.param_groups[0]['lr']
            critic_lr = critic_optimizer.param_groups[0]['lr']

            if wandb is not None:
                wandb.log({
                    "actor_loss": actor_loss_val,
                    "critic_loss": critic_loss_val,
                    "reward": reward_val,
                    "kl": kl_val,
                    "kl_ref": kl_ref_val,
                    "avg_response_len": avg_len_val,
                    "actor_lr": actor_lr,
                })

            Logger(f"Epoch: {epoch+1}, Step: {step}/{iters}, "
                   f"Actor Loss: {actor_loss_val:.6f}, Critic Loss: {critic_loss_val:.6f}, "
                   f"Reward: {reward_val:.6f}, KL: {kl_val:.6f}, KL_ref: {kl_ref_val:.6f}, "
                   f"Avg Response Len: {avg_len_val:.2f}, Actor LR: {actor_lr:.2e}, Critic LR: {critic_lr:.2e}")

        # ğŸ“š æ›´æ–°old actor
        if (step + 1) % args.update_old_actor_freq == 0:
            state_dict = actor_model.module.state_dict() if isinstance(actor_model, DistributedDataParallel) else actor_model.state_dict()
            old_actor_model.load_state_dict({k: v.detach().cpu() for k, v in state_dict.items()})
            old_actor_model.to(args.device)

        # ğŸ“š æ¨¡å‹ä¿å­˜
        if (step % args.save_interval == 0 or step == iters - 1) and is_main_process():
            actor_model.eval()
            moe_suffix = '_moe' if lm_config.use_moe else ''
            ckp = f'{args.save_dir}/{args.save_weight}_{lm_config.hidden_size}{moe_suffix}.pth'
            actor_state = actor_model.module.state_dict() if isinstance(actor_model, DistributedDataParallel) else actor_model.state_dict()
            torch.save({k: v.half() for k, v in actor_state.items()}, ckp)
            
            # ä½¿ç”¨ lm_checkpoint ä¿å­˜å®Œæ•´çŠ¶æ€ï¼ˆåŒ…æ‹¬ criticï¼‰
            lm_checkpoint(lm_config, weight=args.save_weight, model=actor_model, optimizer=actor_optimizer, 
                         epoch=epoch, step=step, wandb=wandb, save_dir='../checkpoints',
                         scheduler=actor_scheduler, critic_model=critic_model, 
                         critic_optimizer=critic_optimizer, critic_scheduler=critic_scheduler)
            actor_model.train()
            


if __name__ == "__main__":
    """
    PPOä¸»å‡½æ•°ï¼šè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–è„šæœ¬çš„å…¥å£ç‚¹
    
    ğŸ“š PPOè®­ç»ƒæ¶æ„ï¼š
    1. Actoræ¨¡å‹ï¼šç”Ÿæˆç­–ç•¥ï¼Œè¾“å‡ºåŠ¨ä½œæ¦‚ç‡
    2. Criticæ¨¡å‹ï¼šä»·å€¼å‡½æ•°ï¼Œä¼°è®¡çŠ¶æ€ä»·å€¼
    3. Rewardæ¨¡å‹ï¼šå¥–åŠ±å‡½æ•°ï¼Œè¯„ä¼°ç”Ÿæˆè´¨é‡
    4. Old Actorï¼šç”¨äºé‡è¦æ€§é‡‡æ ·çš„æ—§ç­–ç•¥
    5. Referenceï¼šç”¨äºKLæƒ©ç½šçš„å‚è€ƒç­–ç•¥
    """
    
    # ğŸ“š å‘½ä»¤è¡Œå‚æ•°è§£æ
    parser = argparse.ArgumentParser(description="MiniMind PPO (Proximal Policy Optimization)")
    
    # ========== åŸºç¡€è®­ç»ƒå‚æ•° ==========
    parser.add_argument("--save_dir", type=str, default="../out", help="æ¨¡å‹ä¿å­˜ç›®å½•")
    parser.add_argument('--save_weight', default='ppo_actor', type=str, help="ä¿å­˜æƒé‡çš„å‰ç¼€å")
    parser.add_argument("--epochs", type=int, default=1, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, default=2, help="batch sizeï¼ˆPPO batchè¾ƒå°ï¼‰")
    
    # ğŸ“š PPOå­¦ä¹ ç‡è®¾ç½®
    # PPOå­¦ä¹ ç‡é€šå¸¸å¾ˆå°ï¼Œé¿å…ç­–ç•¥å‰§çƒˆå˜åŒ–
    parser.add_argument("--learning_rate", type=float, default=8e-8, help="Actorå­¦ä¹ ç‡")
    parser.add_argument("--critic_learning_rate", type=float, default=8e-8, help="Criticå­¦ä¹ ç‡")
    
    # ========== ç¡¬ä»¶é…ç½® ==========
    parser.add_argument("--device", type=str, default="cuda:0" if torch.cuda.is_available() else "cpu", help="è®­ç»ƒè®¾å¤‡")
    parser.add_argument("--dtype", type=str, default="bfloat16", help="æ··åˆç²¾åº¦ç±»å‹")
    parser.add_argument("--num_workers", type=int, default=1, help="æ•°æ®åŠ è½½çº¿ç¨‹æ•°")
    
    # ========== è®­ç»ƒç­–ç•¥ ==========
    parser.add_argument("--accumulation_steps", type=int, default=1, help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°")
    parser.add_argument("--grad_clip", type=float, default=1.0, help="æ¢¯åº¦è£å‰ªé˜ˆå€¼")
    parser.add_argument("--log_interval", type=int, default=1, help="æ—¥å¿—æ‰“å°é—´éš”")
    parser.add_argument("--save_interval", type=int, default=10, help="æ¨¡å‹ä¿å­˜é—´éš”")
    
    # ========== æ¨¡å‹æ¶æ„å‚æ•° ==========
    parser.add_argument('--hidden_size', default=512, type=int, help="éšè—å±‚ç»´åº¦")
    parser.add_argument('--num_hidden_layers', default=8, type=int, help="éšè—å±‚æ•°é‡")
    parser.add_argument('--use_moe', default=0, type=int, choices=[0, 1], help="æ˜¯å¦ä½¿ç”¨MoEæ¶æ„ï¼ˆ0=å¦ï¼Œ1=æ˜¯ï¼‰")
    
    # ========== PPOç”Ÿæˆå‚æ•° ==========
    parser.add_argument('--max_seq_len', default=66, type=int, help="Promptæœ€å¤§é•¿åº¦")
    parser.add_argument("--max_gen_len", type=int, default=1536, help="ç”Ÿæˆçš„æœ€å¤§é•¿åº¦")
    
    # ========== æ•°æ®å’Œæ¨¡å‹å‚æ•° ==========
    parser.add_argument("--data_path", type=str, default="../dataset/rlaif-mini.jsonl", help="RLAIFæ•°æ®è·¯å¾„")
    
    # ğŸ“š PPOè¶…å‚æ•°
    parser.add_argument("--clip_epsilon", type=float, default=0.1, help="PPOè£å‰ªå‚æ•°ï¼ˆæ§åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦ï¼‰")
    parser.add_argument("--vf_coef", type=float, default=0.5, help="Value functionç³»æ•°")
    parser.add_argument("--kl_coef", type=float, default=0.02, help="KLæ•£åº¦æƒ©ç½šç³»æ•°")
    
    # ğŸ“š æ¨ç†æ¨¡å‹é…ç½®
    parser.add_argument("--reasoning", type=int, default=1, choices=[0, 1], help='æ¨ç†æ¨¡å‹ç±»å‹ï¼ˆ0=æ™®é€šæ¨¡å‹ï¼Œ1=æ¨ç†æ¨¡å‹ï¼‰')
    parser.add_argument("--update_old_actor_freq", type=int, default=4, help="æ›´æ–°old_actor_modelçš„é¢‘ç‡")
    
    # ğŸ“š Rewardæ¨¡å‹è·¯å¾„
    parser.add_argument("--reward_model_path", type=str, default="../../internlm2-1_8b-reward", help="Rewardæ¨¡å‹è·¯å¾„")
    
    parser.add_argument('--from_resume', default=0, type=int, choices=[0, 1], help="æ˜¯å¦è‡ªåŠ¨æ£€æµ‹&ç»­è®­ï¼ˆ0=å¦ï¼Œ1=æ˜¯ï¼‰")
    
    # ========== å®éªŒè·Ÿè¸ª ==========
    parser.add_argument("--use_wandb", action="store_true", help="æ˜¯å¦ä½¿ç”¨wandb")
    parser.add_argument("--wandb_project", type=str, default="MiniMind-PPO", help="wandbé¡¹ç›®å")
    
    args = parser.parse_args()
    # ========== 1. åˆå§‹åŒ–ç¯å¢ƒå’Œéšæœºç§å­ ==========
    local_rank = init_distributed_mode()
    if dist.is_initialized(): args.device = f"cuda:{local_rank}"
    setup_seed(42 + (dist.get_rank() if dist.is_initialized() else 0))
    
    # ========== 2. é…ç½®ç›®å½•ã€æ¨¡å‹å‚æ•°ã€æ£€æŸ¥ckp ==========
    os.makedirs(args.save_dir, exist_ok=True)
    lm_config = MokioMindConfig(hidden_size=args.hidden_size, num_hidden_layers=args.num_hidden_layers, use_moe=bool(args.use_moe))
    ckp_data = lm_checkpoint(lm_config, weight=args.save_weight, save_dir='../checkpoints') if args.from_resume==1 else None
    
    # ========== 3. è®¾ç½®æ··åˆç²¾åº¦ ==========
    device_type = "cuda" if "cuda" in args.device else "cpu"
    dtype = torch.bfloat16 if args.dtype == "bfloat16" else torch.float16
    autocast_ctx = nullcontext() if device_type == "cpu" else torch.cuda.amp.autocast(dtype=dtype)
    
    # ========== 4. é…ç½®wandb ==========
    wandb = None
    if args.use_wandb and is_main_process():
        import swanlab as wandb
        wandb_id = ckp_data.get('wandb_id') if ckp_data else None
        resume = 'must' if wandb_id else None
        wandb_run_name = f"MiniMind-PPO-Epoch-{args.epochs}-BS-{args.batch_size}-LR-{args.learning_rate}"
        wandb.init(project=args.wandb_project, name=wandb_run_name, id=wandb_id, resume=resume)
    # ========== 5. åˆå§‹åŒ–æ¨¡å‹å’Œæ•°æ® ==========
    # ğŸ“š PPOæ¨¡å‹æ¶æ„
    base_weight = "reason" if args.reasoning == 1 else "full_sft"
    
    # ğŸ“š Actoræ¨¡å‹ï¼ˆç­–ç•¥æ¨¡å‹ï¼‰
    actor_model, tokenizer = init_model(lm_config, base_weight, device=args.device)
    tokenizer.padding_side = 'left'  # PPOéœ€è¦å·¦ä¾§padding
    
    # ğŸ“š Old Actoræ¨¡å‹ï¼ˆç”¨äºé‡è¦æ€§é‡‡æ ·ï¼‰
    old_actor_model, _ = init_model(lm_config, base_weight, device=args.device)
    old_actor_model = old_actor_model.eval().requires_grad_(False)
    
    # ğŸ“š Referenceæ¨¡å‹ï¼ˆç”¨äºKLæƒ©ç½šï¼‰
    ref_model, _ = init_model(lm_config, base_weight, device=args.device)
    ref_model = ref_model.eval().requires_grad_(False)
    
    # ğŸ“š Criticæ¨¡å‹ï¼ˆä»·å€¼å‡½æ•°ï¼‰
    moe_suffix = '_moe' if lm_config.use_moe else ''
    ckp = f'{args.save_dir}/{base_weight}_{lm_config.hidden_size}{moe_suffix}.pth'
    state_dict = torch.load(ckp, map_location=args.device)
    critic_model = CriticModel(lm_config)
    critic_model.load_state_dict(state_dict, strict=False)
    critic_model = critic_model.to(args.device)
    
    # ğŸ“š Rewardæ¨¡å‹ï¼ˆå¥–åŠ±å‡½æ•°ï¼‰
    reward_model = AutoModel.from_pretrained(
        args.reward_model_path, torch_dtype=torch.float16, trust_remote_code=True
    )
    reward_model = reward_model.to(args.device).eval().requires_grad_(False)
    reward_tokenizer = AutoTokenizer.from_pretrained(args.reward_model_path, trust_remote_code=True)
    
    # ğŸ“š æ•°æ®å’Œä¼˜åŒ–å™¨
    train_ds = RLAIFDataset(args.data_path, tokenizer, max_length=(args.max_seq_len + args.max_gen_len))
    train_sampler = DistributedSampler(train_ds) if dist.is_initialized() else None
    actor_optimizer = optim.AdamW(actor_model.parameters(), lr=args.learning_rate)
    critic_optimizer = optim.AdamW(critic_model.parameters(), lr=args.critic_learning_rate)
    loader_for_count = DataLoader(train_ds, batch_size=args.batch_size, sampler=train_sampler)
    iters = len(loader_for_count)
    total_optimizer_steps = (iters // args.accumulation_steps) * args.epochs
    actor_scheduler = CosineAnnealingLR(actor_optimizer, T_max=total_optimizer_steps, eta_min=args.learning_rate / 10)
    critic_scheduler = CosineAnnealingLR(critic_optimizer, T_max=total_optimizer_steps, eta_min=args.critic_learning_rate / 10)
    
    # ========== 6. ä»ckpæ¢å¤çŠ¶æ€ ==========
    start_epoch, start_step = 0, 0
    if ckp_data:
        actor_model.load_state_dict(ckp_data['model'])
        critic_model.load_state_dict(ckp_data['critic_model'])
        actor_optimizer.load_state_dict(ckp_data['optimizer'])
        critic_optimizer.load_state_dict(ckp_data['critic_optimizer'])
        actor_scheduler.load_state_dict(ckp_data['scheduler'])
        critic_scheduler.load_state_dict(ckp_data['critic_scheduler'])
        start_epoch = ckp_data['epoch']
        start_step = ckp_data.get('step', 0)
    
    # ========== 7. DDPåŒ…è£…æ¨¡å‹ ==========
    if dist.is_initialized():
        actor_model._ddp_params_and_buffers_to_ignore = {"freqs_cos", "freqs_sin"}
        critic_model._ddp_params_and_buffers_to_ignore = {"freqs_cos", "freqs_sin"}
        actor_model = DistributedDataParallel(actor_model, device_ids=[local_rank])
        critic_model = DistributedDataParallel(critic_model, device_ids=[local_rank])
        old_actor_model.to(args.device)
    
    # ========== 8. å¼€å§‹è®­ç»ƒ ==========
    for epoch in range(start_epoch, args.epochs):
        train_sampler and train_sampler.set_epoch(epoch)
        if epoch == start_epoch and start_step > 0:  # ç¬¬ä¸€ä¸ªepochä¸”å­˜åœ¨æ£€æŸ¥ç‚¹
            batch_sampler = SkipBatchSampler(train_sampler or range(len(train_ds)), args.batch_size, start_step + 1)
            loader = DataLoader(train_ds, batch_sampler=batch_sampler, num_workers=args.num_workers, pin_memory=True)
            Logger(f'Epoch [{epoch + 1}/{args.epochs}]: è·³è¿‡å‰{start_step}ä¸ªstepï¼Œä»step {start_step + 1}å¼€å§‹')
            ppo_train_epoch(epoch, loader, len(loader) + start_step + 1, old_actor_model, ref_model, 
                           actor_scheduler, critic_scheduler, reward_model, reward_tokenizer, start_step, wandb)
        else:  # é»˜è®¤ä»å¤´å¼€å§‹
            loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=(train_sampler is None), 
                              sampler=train_sampler, num_workers=args.num_workers, pin_memory=True)
            ppo_train_epoch(epoch, loader, len(loader), old_actor_model, ref_model, 
                           actor_scheduler, critic_scheduler, reward_model, reward_tokenizer, 0, wandb)