"""
MiniMind LoRA (Low-Rank Adaptation) å¾®è°ƒè„šæœ¬

ğŸ“š LoRA æ ¸å¿ƒçŸ¥è¯†ç‚¹ï¼š
- ä»€ä¹ˆæ˜¯LoRAï¼šä¸€ç§å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œåªè®­ç»ƒå°‘é‡æ–°å¢å‚æ•°
- åŸç†ï¼šåœ¨é¢„è®­ç»ƒæ¨¡å‹çš„æƒé‡çŸ©é˜µæ—è¾¹æ·»åŠ ä½ç§©åˆ†è§£çŸ©é˜µ Î”W = BA
  - åŸå§‹æƒé‡ W ä¿æŒå†»ç»“ï¼ˆrequires_grad=Falseï¼‰
  - æ–°å¢ä¸¤ä¸ªå°çŸ©é˜µ A(dÃ—r) å’Œ B(rÃ—d)ï¼Œå…¶ä¸­ r<<dï¼ˆç§©è¿œå°äºç»´åº¦ï¼‰
  - å‰å‘è®¡ç®—ï¼šoutput = Wx + BAx
- ä¼˜åŠ¿å¯¹æ¯”ï¼š
  - Full SFTï¼šæ›´æ–°æ‰€æœ‰å‚æ•°ï¼Œæ•ˆæœå¥½ä½†éœ€è¦å¤§æ˜¾å­˜å’Œé•¿æ—¶é—´
  - LoRAï¼šåªæ›´æ–°1-5%çš„å‚æ•°ï¼Œæ˜¾å­˜éœ€æ±‚å°ï¼Œè®­ç»ƒå¿«ï¼Œé€‚åˆèµ„æºå—é™åœºæ™¯
  - å¤šä»»åŠ¡åˆ‡æ¢ï¼šå¯ä»¥ä¿å­˜å¤šç»„LoRAæƒé‡ï¼Œå¿«é€Ÿåˆ‡æ¢ä¸åŒä»»åŠ¡èƒ½åŠ›

ğŸ“š é€‚ç”¨åœºæ™¯ï¼š
- ä¸ªæ€§åŒ–å®šåˆ¶ï¼šåŒ»ç–—ã€æ³•å¾‹ã€é‡‘èç­‰å‚ç›´é¢†åŸŸé€‚é…
- å¿«é€Ÿå®éªŒï¼šå°è¯•ä¸åŒæ•°æ®/è¶…å‚æ—¶ï¼ŒLoRAè®­ç»ƒé€Ÿåº¦å¿«
- èµ„æºå—é™ï¼šå•å¡æˆ–å°æ˜¾å­˜ç¯å¢ƒ
"""

import os
import sys

# ğŸ“š Pythonæ¨¡å—ç³»ç»Ÿ
# __package__: æ˜¾å¼å£°æ˜å½“å‰æ¨¡å—æ‰€å±çš„åŒ…
# sys.path.append: å°†é¡¹ç›®æ ¹ç›®å½•åŠ å…¥æ¨¡å—æœç´¢è·¯å¾„ï¼Œä½¿å¾—å¯ä»¥å¯¼å…¥projectå†…çš„æ¨¡å—
__package__ = "trainer"
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import argparse  # å‘½ä»¤è¡Œå‚æ•°è§£æ
import time      # æ—¶é—´ç»Ÿè®¡
import warnings  # è­¦å‘Šæ§åˆ¶
import torch     # PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶
import torch.distributed as dist  # åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ
from contextlib import nullcontext  # ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼ˆæ— æ“ä½œå ä½ç¬¦ï¼‰
from torch import optim, nn         # ä¼˜åŒ–å™¨å’Œç¥ç»ç½‘ç»œæ¨¡å—
from torch.nn.parallel import DistributedDataParallel  # åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œ
from torch.utils.data import DataLoader, DistributedSampler  # æ•°æ®åŠ è½½

# MiniMindç›¸å…³ç»„ä»¶
from model.model_minimind import MiniMindConfig     # æ¨¡å‹é…ç½®
from dataset.lm_dataset import SFTDataset          # ç›‘ç£å¾®è°ƒæ•°æ®é›†
from model.model_lora import save_lora, apply_lora # LoRAæƒé‡ä¿å­˜å’Œåº”ç”¨
from trainer.trainer_utils import (                # è®­ç»ƒå·¥å…·å‡½æ•°
    get_lr, Logger, is_main_process, lm_checkpoint, 
    init_distributed_mode, setup_seed, init_model, SkipBatchSampler
)

# å¿½ç•¥è­¦å‘Šä¿¡æ¯ï¼Œä¿æŒè¾“å‡ºæ¸…æ´
warnings.filterwarnings('ignore')


def train_epoch(epoch, loader, iters, lora_params, start_step=0, wandb=None):
    """
    æ‰§è¡Œå•ä¸ªLoRAè®­ç»ƒè½®æ¬¡
    
    ğŸ“š LoRAè®­ç»ƒçš„ç‰¹æ®Šä¹‹å¤„ï¼š
    1. åªæœ‰LoRAå‚æ•°å‚ä¸æ¢¯åº¦è®¡ç®—å’Œæ›´æ–°
    2. åŸå§‹æ¨¡å‹æƒé‡ä¿æŒå†»ç»“ï¼ŒèŠ‚çœæ˜¾å­˜å’Œè®¡ç®—
    3. è®­ç»ƒæµç¨‹ä¸Full SFTç›¸åŒï¼Œä½†å‚æ•°é‡å°å¾—å¤š
    
    Args:
        epoch: å½“å‰è®­ç»ƒè½®æ¬¡
        loader: æ•°æ®åŠ è½½å™¨
        iters: æ€»è¿­ä»£æ¬¡æ•°
        lora_params: LoRAå‚æ•°åˆ—è¡¨ï¼ˆåªæœ‰è¿™äº›å‚æ•°ä¼šè¢«æ›´æ–°ï¼‰
        start_step: èµ·å§‹æ­¥æ•°ï¼ˆç”¨äºæ–­ç‚¹ç»­è®­ï¼‰
        wandb: å®éªŒè·Ÿè¸ªå·¥å…·
    """
    # ğŸ“š äº¤å‰ç†µæŸå¤±å‡½æ•°
    # reduction='none': ä¿æŒæ¯ä¸ªä½ç½®çš„æŸå¤±å€¼ï¼Œä¸è‡ªåŠ¨æ±‚å¹³å‡
    # è¿™æ ·å¯ä»¥é…åˆloss_maskè¿›è¡Œç²¾ç¡®çš„æŸå¤±è®¡ç®—
    loss_fct = nn.CrossEntropyLoss(reduction='none')
    start_time = time.time()
    
    # ğŸ“š enumerateçš„startå‚æ•°
    # start=start_step + 1: ä»æŒ‡å®šæ­¥æ•°å¼€å§‹è®¡æ•°ï¼Œç”¨äºæ–­ç‚¹ç»­è®­æ—¶ä¿æŒstepç¼–å·è¿ç»­
    for step, (X, Y, loss_mask) in enumerate(loader, start=start_step + 1):
        # ğŸ“š å¼ é‡è®¾å¤‡è¿ç§»
        # .to(device): å°†CPUä¸Šçš„å¼ é‡ç§»åŠ¨åˆ°GPUï¼Œå¿…é¡»ä¿è¯æ•°æ®å’Œæ¨¡å‹åœ¨åŒä¸€è®¾å¤‡
        X = X.to(args.device)          # è¾“å…¥åºåˆ—
        Y = Y.to(args.device)          # ç›®æ ‡åºåˆ—
        loss_mask = loss_mask.to(args.device)  # æŸå¤±æ©ç 
        
        # ğŸ“š åŠ¨æ€å­¦ä¹ ç‡è°ƒæ•´
        # ä½¿ç”¨ä½™å¼¦é€€ç«ç­–ç•¥ï¼Œå­¦ä¹ ç‡éšè®­ç»ƒè¿›åº¦å¹³æ»‘ä¸‹é™
        lr = get_lr(epoch * iters + step, args.epochs * iters, args.learning_rate)
        
        # ğŸ“š ä¼˜åŒ–å™¨å‚æ•°ç»„
        # optimizer.param_groups: åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå‚æ•°ç»„å­—å…¸
        # é€šè¿‡ä¿®æ”¹'lr'é”®æ¥åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡
        for param_group in optimizer.param_groups:
            param_group['lr'] = lr

        # ğŸ“š æ··åˆç²¾åº¦è®­ç»ƒä¸Šä¸‹æ–‡
        # autocast_ctx: è‡ªåŠ¨æ··åˆç²¾åº¦ï¼Œå…³é”®è¿ç®—ç”¨float32ï¼Œå…¶ä»–ç”¨float16/bfloat16
        # å¯ä»¥åŠ é€Ÿè®­ç»ƒå¹¶èŠ‚çœæ˜¾å­˜ï¼ŒåŒæ—¶ä¿æŒæ•°å€¼ç¨³å®šæ€§
        with autocast_ctx:
            # æ¨¡å‹å‰å‘ä¼ æ’­
            res = model(X)
            
            # ğŸ“š æŸå¤±è®¡ç®—è¯¦è§£
            # 1. res.logits: [batch_size, seq_len, vocab_size]
            # 2. view(-1, size): å°†å¼ é‡å±•å¹³ï¼Œ-1è¡¨ç¤ºè‡ªåŠ¨è®¡ç®—è¯¥ç»´åº¦
            # 3. Y.view(-1): å°†ç›®æ ‡åºåˆ—å±•å¹³ä¸ºä¸€ç»´
            loss = loss_fct(
                res.logits.view(-1, res.logits.size(-1)),  # [batch*seq, vocab]
                Y.view(-1)                                 # [batch*seq]
            ).view(Y.size())  # æ¢å¤ä¸º [batch_size, seq_len]
            
            # ğŸ“š æ©ç æŸå¤±è®¡ç®—
            # åªå¯¹æœ‰æ•ˆä½ç½®ï¼ˆépaddingï¼‰è®¡ç®—æŸå¤±
            # .sum(): æ±‚å’Œæ‰€æœ‰æœ‰æ•ˆä½ç½®çš„æŸå¤±
            # / loss_mask.sum(): é™¤ä»¥æœ‰æ•ˆä½ç½®æ•°é‡ï¼Œå¾—åˆ°å¹³å‡æŸå¤±
            loss = (loss * loss_mask).sum() / loss_mask.sum()
            
            # ğŸ“š MoEè¾…åŠ©æŸå¤±
            # å¦‚æœä½¿ç”¨MoEï¼ˆæ··åˆä¸“å®¶ï¼‰æ¶æ„ï¼Œéœ€è¦åŠ ä¸Šè´Ÿè½½å‡è¡¡æŸå¤±
            # ç¡®ä¿ä¸åŒä¸“å®¶è¢«å‡åŒ€ä½¿ç”¨
            loss += res.aux_loss
            
            # ğŸ“š æ¢¯åº¦ç´¯ç§¯
            # å°†æŸå¤±é™¤ä»¥ç´¯ç§¯æ­¥æ•°ï¼Œå®ç°æ¢¯åº¦ç´¯ç§¯æ•ˆæœ
            # ç­‰ä»·äºä½¿ç”¨æ›´å¤§çš„batch sizeï¼Œä½†æ˜¾å­˜å ç”¨æ›´å°
            loss = loss / args.accumulation_steps

        # ğŸ“š æ··åˆç²¾åº¦åå‘ä¼ æ’­
        # scaler.scale(loss): æ”¾å¤§æŸå¤±å€¼ï¼Œé˜²æ­¢float16ä¸‹çš„æ¢¯åº¦ä¸‹æº¢
        # .backward(): è®¡ç®—æ¢¯åº¦ï¼Œå¡«å……åˆ°å„å‚æ•°çš„.gradå±æ€§
        scaler.scale(loss).backward()

        # ğŸ“š æ¢¯åº¦ç´¯ç§¯å’Œå‚æ•°æ›´æ–°
        # æ¯accumulation_stepsæ­¥æ‰çœŸæ­£æ›´æ–°ä¸€æ¬¡å‚æ•°
        if (step + 1) % args.accumulation_steps == 0:
            # ğŸ“š æ¢¯åº¦åç¼©æ”¾
            # scaler.unscale_(optimizer): å°†æ”¾å¤§çš„æ¢¯åº¦æ¢å¤åˆ°çœŸå®å€¼
            # å¿…é¡»åœ¨æ¢¯åº¦è£å‰ªä¹‹å‰è°ƒç”¨
            scaler.unscale_(optimizer)
            
            # ğŸ“š æ¢¯åº¦è£å‰ª
            # clip_grad_norm_: å°†æ¢¯åº¦çš„L2èŒƒæ•°é™åˆ¶åœ¨æŒ‡å®šé˜ˆå€¼å†…
            # é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼Œç¨³å®šè®­ç»ƒè¿‡ç¨‹
            # æ³¨æ„ï¼šè¿™é‡Œåªè£å‰ªlora_paramsï¼Œå› ä¸ºå…¶ä»–å‚æ•°å·²è¢«å†»ç»“
            torch.nn.utils.clip_grad_norm_(lora_params, args.grad_clip)

            # ğŸ“š ä¼˜åŒ–å™¨æ­¥è¿›
            # scaler.step(optimizer): æ‰§è¡Œå‚æ•°æ›´æ–° param = param - lr * grad
            # scaler.update(): æ›´æ–°scalerçš„ç¼©æ”¾å› å­ï¼Œç”¨äºä¸‹ä¸€æ¬¡è¿­ä»£
            scaler.step(optimizer)
            scaler.update()

            # ğŸ“š æ¢¯åº¦æ¸…é›¶
            # set_to_none=True: å°†æ¢¯åº¦è®¾ä¸ºNoneè€Œä¸æ˜¯0
            # ä¼˜ç‚¹ï¼šèŠ‚çœå†…å­˜ï¼Œæ€§èƒ½æ›´å¥½
            optimizer.zero_grad(set_to_none=True)

        # ğŸ“š è®­ç»ƒæ—¥å¿—è®°å½•
        # æ¯log_intervalæ­¥æˆ–æœ€åä¸€æ­¥æ‰“å°ä¸€æ¬¡æ—¥å¿—
        if step % args.log_interval == 0 or step == iters - 1:
            spend_time = time.time() - start_time
            # ğŸ“š .item()æ–¹æ³•
            # å°†å•å…ƒç´ å¼ é‡è½¬æ¢ä¸ºPythonæ ‡é‡
            # å¿…é¡»æ¢å¤æ¢¯åº¦ç´¯ç§¯çš„ç¼©æ”¾ï¼šä¹˜ä»¥accumulation_steps
            current_loss = loss.item() * args.accumulation_steps
            current_lr = optimizer.param_groups[-1]['lr']  # è·å–å½“å‰å­¦ä¹ ç‡
            
            # ğŸ“š ETAè®¡ç®—ï¼ˆé¢„è®¡å‰©ä½™æ—¶é—´ï¼‰
            # (å·²ç”¨æ—¶é—´ / å·²å®Œæˆæ­¥æ•°) * æ€»æ­¥æ•° = é¢„è®¡æ€»æ—¶é—´
            # é¢„è®¡æ€»æ—¶é—´ - å·²ç”¨æ—¶é—´ = é¢„è®¡å‰©ä½™æ—¶é—´
            eta_min = spend_time / (step + 1) * iters // 60 - spend_time // 60
            
            # ğŸ“š f-stringæ ¼å¼åŒ–
            # {var:.6f}: ä¿ç•™6ä½å°æ•°
            # {var:.12f}: ä¿ç•™12ä½å°æ•°ï¼ˆå­¦ä¹ ç‡é€šå¸¸å¾ˆå°ï¼‰
            Logger(f'Epoch:[{epoch+1}/{args.epochs}]({step}/{iters}) loss:{current_loss:.6f} lr:{current_lr:.12f} epoch_Time:{eta_min}min:')
            
            # è®°å½•åˆ°å®éªŒè·Ÿè¸ªç³»ç»Ÿ
            if wandb: 
                # ğŸ“š wandb.log()
                # è®°å½•æ ‡é‡æŒ‡æ ‡åˆ°WandB/SwanLabå¹³å°
                # å¯ä»¥åœ¨ç½‘é¡µç«¯å®æ—¶æŸ¥çœ‹è®­ç»ƒæ›²çº¿
                wandb.log({"loss": current_loss, "lr": current_lr, "epoch_Time": eta_min})

        # ğŸ“š LoRAæ¨¡å‹æ£€æŸ¥ç‚¹ä¿å­˜
        # æ¯save_intervalæ­¥æˆ–æœ€åä¸€æ­¥ä¿å­˜ä¸€æ¬¡
        # is_main_process(): åªæœ‰ä¸»è¿›ç¨‹ä¿å­˜ï¼Œé¿å…å¤šè¿›ç¨‹é‡å¤å†™å…¥
        if (step % args.save_interval == 0 or step == iters - 1) and is_main_process():
            model.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
            
            # ğŸ“š LoRAæƒé‡ä¿å­˜è·¯å¾„
            # åªä¿å­˜LoRAçš„Aå’ŒBçŸ©é˜µï¼Œä¸ä¿å­˜æ•´ä¸ªæ¨¡å‹
            lora_save_path = f'{args.save_dir}/{args.lora_name}_{lm_config.hidden_size}.pth'
            
            # ğŸ“š save_loraå‡½æ•°
            # ä»æ¨¡å‹ä¸­æå–æ‰€æœ‰åŒ…å«'lora'çš„å‚æ•°å¹¶ä¿å­˜
            # æ–‡ä»¶å¤§å°é€šå¸¸åªæœ‰Full SFTçš„1-5%
            save_lora(model, lora_save_path)
            
            # ğŸ“š å®Œæ•´è®­ç»ƒçŠ¶æ€ä¿å­˜
            # ä¿å­˜æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€scalerã€è®­ç»ƒè¿›åº¦ç­‰
            # ç”¨äºæ–­ç‚¹ç»­è®­
            lm_checkpoint(lm_config, weight=args.lora_name, model=model, 
                         optimizer=optimizer, scaler=scaler, epoch=epoch, 
                         step=step, wandb=wandb, save_dir='../checkpoints')
            
            model.train()  # æ¢å¤è®­ç»ƒæ¨¡å¼


if __name__ == "__main__":
    # ğŸ“š å‘½ä»¤è¡Œå‚æ•°è§£æ
    # argparse: Pythonæ ‡å‡†åº“ï¼Œç”¨äºè§£æå‘½ä»¤è¡Œå‚æ•°
    # æä¾›é»˜è®¤å€¼å’Œå¸®åŠ©ä¿¡æ¯ï¼Œä¾¿äºç”¨æˆ·é…ç½®è®­ç»ƒå‚æ•°
    parser = argparse.ArgumentParser(description="MiniMind LoRA Fine-tuning")
    
    # ğŸ“š æ¨¡å‹ä¿å­˜ç›¸å…³å‚æ•°
    # save_dir: æŒ‡å®šLoRAæƒé‡å’Œæ£€æŸ¥ç‚¹çš„ä¿å­˜ç›®å½•
    # lora_name: LoRAæƒé‡çš„æ ‡è¯†ç¬¦ï¼Œç”¨äºåŒºåˆ†ä¸åŒä»»åŠ¡çš„LoRAé€‚é…å™¨
    parser.add_argument("--save_dir", type=str, default="../out/lora", help="æ¨¡å‹ä¿å­˜ç›®å½•")
    parser.add_argument("--lora_name", type=str, default="lora_identity", help="LoRAæƒé‡åç§°(å¦‚lora_identity/lora_medicalç­‰)")
    
    # ğŸ“š è®­ç»ƒè¶…å‚æ•°
    # epochs: è®­ç»ƒçš„æ€»è½®æ•°ï¼Œæ§åˆ¶æ¨¡å‹è®­ç»ƒçš„å®Œæ•´ç¨‹åº¦
    # batch_size: æ¯ä¸ªæ‰¹æ¬¡çš„æ ·æœ¬æ•°é‡ï¼Œå½±å“æ˜¾å­˜ä½¿ç”¨å’Œè®­ç»ƒç¨³å®šæ€§
    # learning_rate: åˆå§‹å­¦ä¹ ç‡ï¼Œæ§åˆ¶å‚æ•°æ›´æ–°çš„æ­¥é•¿
    parser.add_argument("--epochs", type=int, default=50, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, default=32, help="batch size")
    parser.add_argument("--learning_rate", type=float, default=1e-4, help="åˆå§‹å­¦ä¹ ç‡")
    
    # ğŸ“š è®¾å¤‡å’Œç²¾åº¦é…ç½®
    # device: æŒ‡å®šè®­ç»ƒä½¿ç”¨çš„è®¾å¤‡ï¼ˆGPU/CPUï¼‰
    # dtype: æ··åˆç²¾åº¦è®­ç»ƒçš„æ•°æ®ç±»å‹ï¼Œbfloat16æ›´ç¨³å®šï¼Œfloat16æ›´é«˜æ•ˆ
    parser.add_argument("--device", type=str, default="cuda:0" if torch.cuda.is_available() else "cpu", help="è®­ç»ƒè®¾å¤‡")
    parser.add_argument("--dtype", type=str, default="bfloat16", help="æ··åˆç²¾åº¦ç±»å‹")
    
    # ğŸ“š æ•°æ®åŠ è½½å’Œè®­ç»ƒä¼˜åŒ–
    # num_workers: æ•°æ®åŠ è½½çš„å¹¶è¡Œè¿›ç¨‹æ•°ï¼Œæé«˜æ•°æ®è¯»å–æ•ˆç‡
    # accumulation_steps: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼Œæ¨¡æ‹Ÿæ›´å¤§çš„batch size
    # grad_clip: æ¢¯åº¦è£å‰ªé˜ˆå€¼ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
    parser.add_argument("--num_workers", type=int, default=1, help="æ•°æ®åŠ è½½çº¿ç¨‹æ•°")
    parser.add_argument("--accumulation_steps", type=int, default=1, help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°")
    parser.add_argument("--grad_clip", type=float, default=1.0, help="æ¢¯åº¦è£å‰ªé˜ˆå€¼")
    
    # ğŸ“š æ—¥å¿—å’Œä¿å­˜é…ç½®
    # log_interval: æ¯å¤šå°‘æ­¥æ‰“å°ä¸€æ¬¡è®­ç»ƒæ—¥å¿—
    # save_interval: æ¯å¤šå°‘æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹æ£€æŸ¥ç‚¹
    parser.add_argument("--log_interval", type=int, default=10, help="æ—¥å¿—æ‰“å°é—´éš”")
    parser.add_argument("--save_interval", type=int, default=1, help="æ¨¡å‹ä¿å­˜é—´éš”")
    
    # ğŸ“š æ¨¡å‹æ¶æ„å‚æ•°
    # hidden_size: æ¨¡å‹éšè—å±‚ç»´åº¦ï¼Œå½±å“æ¨¡å‹å®¹é‡å’Œè®¡ç®—å¤æ‚åº¦
    # num_hidden_layers: Transformerå±‚æ•°ï¼Œå±‚æ•°è¶Šå¤šæ¨¡å‹è¶Šæ·±
    # max_seq_len: è®­ç»ƒæ—¶åºåˆ—çš„æœ€å¤§é•¿åº¦ï¼Œå½±å“æ˜¾å­˜ä½¿ç”¨
    # use_moe: æ˜¯å¦ä½¿ç”¨Mixture of Expertsæ¶æ„ï¼Œæé«˜æ¨¡å‹æ•ˆç‡
    parser.add_argument('--hidden_size', default=512, type=int, help="éšè—å±‚ç»´åº¦")
    parser.add_argument('--num_hidden_layers', default=8, type=int, help="éšè—å±‚æ•°é‡")
    parser.add_argument('--max_seq_len', default=512, type=int, help="è®­ç»ƒçš„æœ€å¤§æˆªæ–­é•¿åº¦")
    parser.add_argument('--use_moe', default=0, type=int, choices=[0, 1], help="æ˜¯å¦ä½¿ç”¨MoEæ¶æ„ï¼ˆ0=å¦ï¼Œ1=æ˜¯ï¼‰")
    
    # ğŸ“š æ•°æ®å’Œæƒé‡é…ç½®
    # data_path: è®­ç»ƒæ•°æ®çš„æ–‡ä»¶è·¯å¾„ï¼Œé€šå¸¸æ˜¯JSONLæ ¼å¼
    # from_weight: åŸºäºå“ªä¸ªé¢„è®­ç»ƒæƒé‡è¿›è¡ŒLoRAå¾®è°ƒ
    # from_resume: æ˜¯å¦ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒï¼Œæ”¯æŒæ–­ç‚¹ç»­è®­
    parser.add_argument("--data_path", type=str, default="../dataset/lora_identity.jsonl", help="LoRAè®­ç»ƒæ•°æ®è·¯å¾„")
    parser.add_argument('--from_weight', default='full_sft', type=str, help="åŸºäºå“ªä¸ªæƒé‡è®­ç»ƒï¼Œé»˜è®¤full_sft")
    parser.add_argument('--from_resume', default=0, type=int, choices=[0, 1], help="æ˜¯å¦è‡ªåŠ¨æ£€æµ‹&ç»­è®­ï¼ˆ0=å¦ï¼Œ1=æ˜¯ï¼‰")
    
    # ğŸ“š å®éªŒè·Ÿè¸ªé…ç½®
    # use_wandb: æ˜¯å¦å¯ç”¨WandB/SwanLabè¿›è¡Œå®éªŒè·Ÿè¸ª
    # wandb_project: WandBé¡¹ç›®çš„åç§°ï¼Œç”¨äºç»„ç»‡å®éªŒ
    parser.add_argument("--use_wandb", action="store_true", help="æ˜¯å¦ä½¿ç”¨wandb")
    parser.add_argument("--wandb_project", type=str, default="MiniMind-LoRA", help="wandbé¡¹ç›®å")
    args = parser.parse_args()

    # ========== 1. åˆå§‹åŒ–ç¯å¢ƒå’Œéšæœºç§å­ ==========
    # ğŸ“š åˆ†å¸ƒå¼è®­ç»ƒåˆå§‹åŒ–
    # init_distributed_mode(): åˆå§‹åŒ–å¤šGPUåˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ
    # å¦‚æœä½¿ç”¨å¤šå¡ï¼Œä¼šè®¾ç½®è¿›ç¨‹ç»„å’Œæœ¬åœ°rank
    local_rank = init_distributed_mode()
    
    # ğŸ“š è®¾å¤‡åˆ†é…
    # åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œæ¯ä¸ªè¿›ç¨‹ä½¿ç”¨ä¸åŒçš„GPU
    # dist.get_rank(): è·å–å½“å‰è¿›ç¨‹çš„å…¨å±€rank
    if dist.is_initialized(): args.device = f"cuda:{local_rank}"
    
    # ğŸ“š éšæœºç§å­è®¾ç½®
    # setup_seed(): è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿è®­ç»ƒçš„å¯å¤ç°æ€§
    # ä¸åŒè¿›ç¨‹ä½¿ç”¨ä¸åŒçš„ç§å­ï¼Œé¿å…ç”Ÿæˆç›¸åŒçš„æ•°æ®
    setup_seed(42 + (dist.get_rank() if dist.is_initialized() else 0))
    
    # ========== 2. é…ç½®ç›®å½•ã€æ¨¡å‹å‚æ•°ã€æ£€æŸ¥ckp ==========
    # ğŸ“š åˆ›å»ºä¿å­˜ç›®å½•
    # os.makedirs: é€’å½’åˆ›å»ºç›®å½•ï¼Œå¦‚æœå·²å­˜åœ¨åˆ™å¿½ç•¥
    os.makedirs(args.save_dir, exist_ok=True)
    
    # ğŸ“š æ¨¡å‹é…ç½®åˆå§‹åŒ–
    # MiniMindConfig: å®šä¹‰æ¨¡å‹çš„è¶…å‚æ•°ï¼Œå¦‚éšè—ç»´åº¦ã€å±‚æ•°ã€æ˜¯å¦ä½¿ç”¨MoE
    lm_config = MiniMindConfig(hidden_size=args.hidden_size, num_hidden_layers=args.num_hidden_layers, use_moe=bool(args.use_moe))
    
    # ğŸ“š æ£€æŸ¥ç‚¹æ£€æµ‹
    # lm_checkpoint(): æ£€æŸ¥æ˜¯å¦å­˜åœ¨å¯ç”¨çš„æ£€æŸ¥ç‚¹
    # å¦‚æœfrom_resume=1ï¼Œåˆ™å°è¯•åŠ è½½ä¹‹å‰çš„è®­ç»ƒçŠ¶æ€
    ckp_data = lm_checkpoint(lm_config, weight=args.lora_name, save_dir='../checkpoints') if args.from_resume==1 else None
    
    # ========== 3. è®¾ç½®æ··åˆç²¾åº¦ ==========
    # ğŸ“š è®¾å¤‡ç±»å‹åˆ¤æ–­
    # æ ¹æ®è®¾å¤‡å­—ç¬¦ä¸²åˆ¤æ–­æ˜¯CPUè¿˜æ˜¯GPU
    device_type = "cuda" if "cuda" in args.device else "cpu"
    
    # ğŸ“š æ•°æ®ç±»å‹é€‰æ‹©
    # bfloat16: æ›´å¥½çš„æ•°å€¼ç¨³å®šæ€§ï¼Œé€‚åˆç°ä»£GPU
    # float16: æ›´é«˜çš„æ€§èƒ½ï¼Œä½†å¯èƒ½æœ‰ç²¾åº¦æŸå¤±
    dtype = torch.bfloat16 if args.dtype == "bfloat16" else torch.float16
    
    # ğŸ“š è‡ªåŠ¨æ··åˆç²¾åº¦ä¸Šä¸‹æ–‡
    # autocast: è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„ç²¾åº¦è¿›è¡Œè®¡ç®—
    # CPUæ¨¡å¼ä¸‹ä½¿ç”¨nullcontextï¼ˆæ— æ“ä½œï¼‰
    autocast_ctx = nullcontext() if device_type == "cpu" else torch.cuda.amp.autocast(dtype=dtype)
    
    # ========== 4. é…ç½®wandb ==========
    # ğŸ“š å®éªŒè·Ÿè¸ªåˆå§‹åŒ–
    # SwanLab: ç±»ä¼¼WandBçš„å®éªŒç®¡ç†å·¥å…·
    # æ”¯æŒå®éªŒé‡å¯å’ŒæŒ‡æ ‡è®°å½•
    wandb = None
    if args.use_wandb and is_main_process():
        import swanlab as wandb
        
        # ğŸ“š WandBè¿è¡ŒID
        # ä»æ£€æŸ¥ç‚¹æ¢å¤æ—¶ä½¿ç”¨ç›¸åŒçš„IDï¼Œä¿æŒå®éªŒè¿ç»­æ€§
        wandb_id = ckp_data.get('wandb_id') if ckp_data else None
        resume = 'must' if wandb_id else None
        
        # ğŸ“š å®éªŒåç§°ç”Ÿæˆ
        # åŒ…å«å…³é”®å‚æ•°ï¼Œä¾¿äºè¯†åˆ«ä¸åŒçš„å®éªŒé…ç½®
        wandb_run_name = f"MiniMind-LoRA-{args.lora_name}-Epoch-{args.epochs}-BatchSize-{args.batch_size}-LR-{args.learning_rate}"
        wandb.init(project=args.wandb_project, name=wandb_run_name, id=wandb_id, resume=resume)
    
    # ========== 5. å®šä¹‰æ¨¡å‹ã€åº”ç”¨LoRAã€å†»ç»“éLoRAå‚æ•° ==========
    # ğŸ“š æ¨¡å‹åˆå§‹åŒ–
    # init_model(): åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œtokenizer
    # from_weightæŒ‡å®šåŸºç¡€æƒé‡æ–‡ä»¶
    model, tokenizer = init_model(lm_config, args.from_weight, device=args.device)
    
    # ğŸ“š åº”ç”¨LoRAé€‚é…å™¨
    # apply_lora(): åœ¨æ¨¡å‹ä¸­æ³¨å…¥LoRAå‚æ•°
    # ä¸ºæŒ‡å®šçš„å±‚æ·»åŠ Aå’ŒBçŸ©é˜µ
    apply_lora(model)
    
    # ğŸ“š å‚æ•°ç»Ÿè®¡
    # è®¡ç®—æ€»å‚æ•°é‡å’ŒLoRAå‚æ•°é‡
    # LoRAå‚æ•°é€šå¸¸åªå æ€»å‚æ•°çš„1-5%
    total_params = sum(p.numel() for p in model.parameters())
    lora_params_count = sum(p.numel() for name, p in model.named_parameters() if 'lora' in name)
    Logger(f"LLM æ€»å‚æ•°é‡: {total_params / 1e6:.3f} M")
    Logger(f"LoRA å‚æ•°é‡: {lora_params_count / 1e6:.3f} M")
    Logger(f"LoRA å‚æ•°å æ¯”: {lora_params_count / total_params * 100:.2f}%")
    
    # ğŸ“š å‚æ•°å†»ç»“ç­–ç•¥
    # å†»ç»“éLoRAå‚æ•°ï¼Œåªè®­ç»ƒLoRAé€‚é…å™¨
    # æ”¶é›†éœ€è¦ä¼˜åŒ–çš„LoRAå‚æ•°åˆ—è¡¨
    lora_params = []
    for name, param in model.named_parameters():
        if 'lora' in name:
            param.requires_grad = True
            lora_params.append(param)
        else:
            param.requires_grad = False
    
    # ========== 6. å®šä¹‰æ•°æ®å’Œä¼˜åŒ–å™¨ ==========
    # ğŸ“š æ•°æ®é›†å‡†å¤‡
    # SFTDataset: ç›‘ç£å¾®è°ƒæ•°æ®é›†ç±»
    # å¤„ç†JSONLæ ¼å¼çš„æ•°æ®ï¼Œè¿›è¡Œtokenizationå’Œæˆªæ–­
    train_ds = SFTDataset(args.data_path, tokenizer, max_length=args.max_seq_len)
    
    # ğŸ“š æ•°æ®é‡‡æ ·å™¨
    # DistributedSampler: åˆ†å¸ƒå¼è®­ç»ƒçš„æ•°æ®é‡‡æ ·å™¨
    # ç¡®ä¿ä¸åŒè¿›ç¨‹é‡‡æ ·ä¸åŒçš„æ•°æ®å­é›†
    train_sampler = DistributedSampler(train_ds) if dist.is_initialized() else None
    
    # ğŸ“š æ¢¯åº¦ç¼©æ”¾å™¨
    # GradScaler: ç”¨äºfloat16è®­ç»ƒçš„æ¢¯åº¦ç¼©æ”¾
    # é˜²æ­¢æ¢¯åº¦ä¸‹æº¢ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§
    scaler = torch.cuda.amp.GradScaler(enabled=(args.dtype == 'float16'))
    
    # ğŸ“š ä¼˜åŒ–å™¨é…ç½®
    # AdamW: å¸¸ç”¨çš„ä¼˜åŒ–å™¨ï¼Œå¸¦æœ‰æƒé‡è¡°å‡
    # åªä¼˜åŒ–LoRAå‚æ•°ï¼ŒåŸå§‹å‚æ•°ä¿æŒå†»ç»“
    optimizer = optim.AdamW(lora_params, lr=args.learning_rate)
    
    # ========== 7. ä»ckpæ¢å¤çŠ¶æ€ ==========
    # ğŸ“š è®­ç»ƒçŠ¶æ€æ¢å¤
    # å¦‚æœå­˜åœ¨æ£€æŸ¥ç‚¹ï¼Œä»ä¸­æ¢å¤æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€scalerçŠ¶æ€
    # æ”¯æŒæ–­ç‚¹ç»­è®­ï¼ŒèŠ‚çœè®­ç»ƒæ—¶é—´
    start_epoch, start_step = 0, 0
    if ckp_data:
        model.load_state_dict(ckp_data['model'], strict=False)
        optimizer.load_state_dict(ckp_data['optimizer'])
        scaler.load_state_dict(ckp_data['scaler'])
        start_epoch = ckp_data['epoch']
        start_step = ckp_data.get('step', 0)
    
    # ========== 8. DDPåŒ…è£…æ¨¡å‹ ==========
    # ğŸ“š åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œ
    # DistributedDataParallel: PyTorchçš„DDPå®ç°
    # å°†æ¨¡å‹åŒ…è£…ä¸ºåˆ†å¸ƒå¼ç‰ˆæœ¬ï¼Œæ”¯æŒå¤šGPUè®­ç»ƒ
    # _ddp_params_and_buffers_to_ignore: å¿½ç•¥ä¸éœ€è¦åŒæ­¥çš„ç¼“å†²åŒºï¼ˆå¦‚ä½ç½®ç¼–ç ï¼‰
    if dist.is_initialized():
        model._ddp_params_and_buffers_to_ignore = {"freqs_cos", "freqs_sin"}
        model = DistributedDataParallel(model, device_ids=[local_rank])
    
    # ========== 9. å¼€å§‹è®­ç»ƒ ==========
    # ğŸ“š è®­ç»ƒå¾ªç¯
    # éå†æ¯ä¸ªepochï¼Œæ‰§è¡Œè®­ç»ƒè¿‡ç¨‹
    # æ”¯æŒä»æ£€æŸ¥ç‚¹æ¢å¤ï¼Œç»§ç»­æœªå®Œæˆçš„è®­ç»ƒ
    for epoch in range(start_epoch, args.epochs):
        # ğŸ“š é‡‡æ ·å™¨epochè®¾ç½®
        # set_epoch(): ç¡®ä¿åˆ†å¸ƒå¼é‡‡æ ·å™¨çš„éšæœºæ€§
        train_sampler and train_sampler.set_epoch(epoch)
        
        if epoch == start_epoch and start_step > 0: # ç¬¬ä¸€ä¸ªepochä¸”å­˜åœ¨æ£€æŸ¥ç‚¹
            # ğŸ“š è·³è¿‡å·²å®Œæˆçš„step
            # SkipBatchSampler: è‡ªå®šä¹‰é‡‡æ ·å™¨ï¼Œè·³è¿‡å‰Nä¸ªbatch
            # ç”¨äºæ–­ç‚¹ç»­è®­æ—¶ä»æŒ‡å®šstepå¼€å§‹
            batch_sampler = SkipBatchSampler(train_sampler or range(len(train_ds)), args.batch_size, start_step + 1)
            loader = DataLoader(train_ds, batch_sampler=batch_sampler, num_workers=args.num_workers, pin_memory=True)
            Logger(f'Epoch [{epoch + 1}/{args.epochs}]: è·³è¿‡å‰{start_step}ä¸ªstepï¼Œä»step {start_step + 1}å¼€å§‹')
            train_epoch(epoch, loader, len(loader) + start_step + 1, lora_params, start_step, wandb)
        else: # é»˜è®¤ä»å¤´å¼€å§‹
            # ğŸ“š æ ‡å‡†æ•°æ®åŠ è½½å™¨
            # DataLoader: PyTorchçš„æ•°æ®åŠ è½½å™¨
            # shuffle: å•GPUæ—¶éšæœºæ‰“ä¹±ï¼Œå¤šGPUæ—¶ç”±sampleræ§åˆ¶
            loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=(train_sampler is None), sampler=train_sampler, num_workers=args.num_workers, pin_memory=True)
            train_epoch(epoch, loader, len(loader), lora_params, 0, wandb)
